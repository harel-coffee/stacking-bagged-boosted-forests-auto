\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\bibliographystyle{plainnat}
\usepackage{longtable,booktabs}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={Raphael Rodrigues Campos},
            pdftitle={Stacking utilizado nos experimentos},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{5}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Stacking utilizado nos experimentos}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Raphael Rodrigues Campos}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{28 abril, 2016}

\usepackage{multirow} \usepackage[utf8]{inputenc} \usepackage{mathtools}
\usepackage[portuguese]{babel}


\begin{document}

\maketitle


\chapter{Overview}

\subsection{Stacking}\label{stacking}

Stacking também conhecido como ``Stacked Generalization'' é um método
para combinar multiplos classificadores usando algoritmos de
aprendizados heterogêneos \(L_1, ..., L_N\) sobre um único conjunto de
dados \(D\), que consiste de exemplos \(e_i = (x_i, y_i)\), onde \(x_i\)
é o vetor de atributos e \(y_i\) sua classificação.

\subsubsection{Stacking Framework}\label{stacking-framework}

O staking framework utilizado é baseado no descrito em {[}1{]} David H.
Wolpert, ``Stacked Generalization'', Neural Networks, 5, 241--259, 1992.
Foi utilizado um stacking de dois níveis (o framework não se limita a
apenas dois níveis, é possível fazer o stacking de quantos níveis julgar
necessário), que pode ser dividido em duas fases. Na primeira fase, um
conjunto de classificadores do nível base \(C_1, C_2, ..., C_N\) é
gerado, onde \(C_i = L_i(D)\). Na segunda fase um classidicador do
meta-nível aprende a combinar as saídas dos classificadores do nível
base.

Para gerar o conjunto de treino para o aprendizado do classificador do
meta-nível, pode-se aplicar o procedimento \textbf{leave-one-out} ou
\textbf{cross validation}. Por questões óbvias de custo computacional, é
utilizado nesse relatório cross validation, mais especificamente
\textbf{5-fold cross validation}. Cada classificador do nível base
aprende usando \(D - F_k\) deixando o k-ésimo \emph{fold} para teste:
\(\forall i = 1,...,N : \forall k = 1,...,5 : C^{k}_i = L_i(D-F_k)\).
Agora, os classificadores recém aprendidos são usados para gerar as
predições para \(\forall x_j \in F_k:\hat{y}_j^i=C^k_i(x_j)\). O
conjunto de treino do meta-nível consiste de exemplos da seguinte forma
\(((\hat{y}_i^1,..., \hat{y}_i^N), y_i)\), onde os atributos são as
predicóes do s classificadores do nível base e a classe é a classe
correta sabida de antemão.

\paragraph{Exemplo}\label{exemplo}

Esse procedimento pode parecer complicado, mas na verdade é simples.
Como um exemplo, vamos gerar alguns dados sintéticos com a função
``saída = soma do três componentes de entrada''. Nosso conjunto de
treino D consiste de 5 pares de entrada e saída
\(\{((0,0,0),0), ((1,0,0),1), ((1,2,0),3), ((1,1,1),3), ((1,-2,4),3)\}\),
todas as entradas sem ruídos. Vamos rotular esses 5 pares de entrada e
saída como \(F_1\) até \(F_5\) (Então por exemplo \(D - F_2\) consiste
dos quatros pares
\(\{((0,0,0),0), ((1,2,0),3), ((1,1,1),3), ((1,-2,4),3)\}\)). Nesse
exemplo, temos dois classificadores do nível base \(C_1\) e \(C_2\), e
um único classificador do meta-nível \(\Gamma\). O conjunto de treino do
meta-nível \(D'\) é dado pelo cinco pares de entrada e saída \(\{\)
((\(C_1^k(F_k), C_2^k(F_k)\)), componente de saída de
\(F_k) : \forall k \in \{1,...,5\}\) e \(C_i^k = L_i(D-F_k)\}\) (Esse
espaço do meta-nível possui duas dimensões de entrada e uma de saída).
Ou seja, a instância do conjunto de treino do meta-nível correspondente
a \(k = 1\) tem o componetne de saída 0 e entrada
\((C_1^1((0,0,0)), C_2^1((0,0,0)))\). Agora nos é dado um exemplo de
teste no formato do nível base \((x_1, x_2, x_3)\). Nós predizemos seu
valor com \(\Gamma((C_1((x_1, x_2, x_3)), (C_2((x_1, x_2, x_3)))\), onde
\(C_1\) e \(C_2\) foram treinados com todo \(D\), e \(\Gamma\) com
\(D'\). Em outras palavras, nós predizemos o valor da entrada de teste
\(q = (x_1, x_2, x_3)\) treinando \(\Gamma\) em \(D'\) e assim
predizendo a entrada formada pelas predições do valor do exemplo de
teste \(q\), de ambos classificadores do nível base \(C_1\) e \(C_2\),
que por suas vezes foram treinados com todo \(D\).

\subsubsection{Stacking com distribuições de
probabilidade}\label{stacking-com-distribuicoes-de-probabilidade}

Usar probabilidade para gerar o conjunto de treino do meta-nível é mais
vantajoso já que disponibiliza mais informação acerca das predições
feitas pelos classificadores do nível base. Essa informações adicionais
permitem que não seja usado somente a predição, mas também o confiança
de cada classificador do nível base.

Nessa abordagem, cada classificador do nível base prediz uma
Distribuição de Probabilidade (DP) sobre todas as classes possíveis.
Então, a predição do classificador do nível base \(C\) apliacado a um
exemplo \(x\) é a DP: \(p^C(x) = (p^C(c_1|x), ... , p^C(c_m|x))\), onde
\(\{c_1, ..., c_m\}\) é o conjunto de possíveis valores para as classes
e \(p^C(c_i|x))\) descreve a probabilidade do exemplo \(x\) ser da
classe \(c_i\) estimado pelo classificador \(C\). A classe \(c_j\) com
maior probalidade será classe predita por \(C\). Dessa forma, os
atributos do meta-nível serão as probabilidade preditas para cada classe
possível por cada classificador do nível base. O número total de
atributos no conjunto de treino do meta-nível seria \(Nm\), \(m\)
atributos para cada classificador do nível base.

Os experimentos rodados até então utilizaram o stacking framework com
DPs.

\subsubsection{Stacking com DP, Entropia e probabilidade
máxima}\label{stacking-com-dp-entropia-e-probabilidade-maxima}

No artigo {[}2{]} Is combining classifiers better than selecting the
best one, os autores propões uma extensão para esse framework com DP
espandindo o número de meta-atributos. Esse novos meta-atributos seriam:

\begin{itemize}
\item
  A distribuição de probabilidade mutiplicadao pela probabilidade
  máxima:
  \(p_{C_j} = p^{C_j}(c_i|x) \times M_{C_j}(x) = p^{C_j}(c_i|x) \times max_{i=1}^{m}(p^{C_j}(c_i|x))\),
  \(\forall i \in \{1,...,m\}\) e \(\forall j \in \{1,...,N\}\).
\item
  As entropias das distripuições de probabilidade:
  \(E_{C_j}(x) = -\sum_{i=1}^{m}p^{C_j}(c_i|x).\log_2(p^{C_j}(c_i|x))\).
\end{itemize}

O número total de atributos do meta-nível é \(N(2m+1)\).

A idea é obter ainda mais informações em relação a predição feita pelos
classificadores do nível base. Como Ting and Witten (1999) disseram: o
uso de distribuição de probabilidades tem a vantagemde capturar não
apenas as predições dos classificadores do nível base, mas também, suas
certezas. Os atributos adicionais tentam capturar a certeza de forma
mais explicita.

Entropia é uma medida de incerteza. Quanto maior a entropia da
distribuição menor é a certeza sobre a predição. A probabilidade máxima
de uma DP \(M_{C_j}\) também contém informação sobre certeza da
predição: quanto maior \(M_{C_j}\) for mais certo daquela resposta o
classificador do nível base está, e vice versa.

Esse é uma ideia para aplicarmos futuramente. Nesse momento continuarei
utilizanto somente a DP.

\section{SVM}\label{svm}

Nessa seção, vamos dar uma visão geral sobre Support Vector Machine(SVM)
utilizado nos experimentos.

Seja \(x_i \in R^d\) um vetor de caracteríticas. Nosso objetivo é
projetar um classificador, por exemplo, que associa a cada vetor \(x_i\)
um rótulo positivo ou negativo baseado no critério desejado.

O vetor \(x_i\) é classificador olhando o sinal do resultado do função
\(f(x_i,w) = w^Tx_i\).O objetivo é aprender a estimar os paramêtros
\(w \in R^d\) de tal forma que o sinal é positvo seo vetor \(x_i\)
pertence a classe negativa e negativo caso contrário. De fato, na
formulação padrão do SVM o objetivo é ter o o valor da função no mínimo
1 no primeiro caso, e no máximo -1* no segundo, impondo uma margem.

O paramêtr \(w\) é estimado ou aprendido ajustando o função a um
conjunto de treino de \(n\) pares de exemplos \((x_i,y_i),i=1,…,n\),
onde \(y_i \in \{-1,1\}\) são os rótulos dos correspondentes vetores de
caracteríticas. A qualidade do ajuste é mensurada pela função de perda
que, em SVMs padrões SVMs, é a \textbf{hinge loss}:

\begin{equation}
l_i(w,x_i)=\max(0,1-y_iw^Tx_i))
\end{equation}

Note que a \textbf{hinge loss} é zero apenas se o valor de \(w^Tx_i\) é
no mínimo 1 ou no máximo -1, dependendo do rótulo de \(y_i\).

Somente ajustar ao treino é normalmente insuficiente. Para que a função
seja capaz de generalizar para dados nunca visto, é preferível um
\textbf{trade off} entre a acurácia do ajuste e complexidade do modelo.
Dessa forma, é adionado um termo de regularição a formula, o
regularizador na formulação padrão é mensurado pela normado vetor de
pesos \(\mid w\mid^2\). Tirando-se a média da perda de todo os exemplos
de treino e adicionando-se a ela o regularizador ponderado pelo
paramêtro \(\lambda\) produz uma função objetiva de perda regularizada:

\begin{equation}
  E(w)  = \lambda||w||^2+\frac{1}{n} \sum_{i=1}^{n}\max(0,1-y_if(w,x)
\end{equation}

Note que a função objetiva é convexa, desse modo existe um único ótimo
global.

A função \(f(x_i,w)\) considerada até aqui é linear em sem viés. A
subseção seguinte discute como um termo de viés pode ser adicionado ao
SVM.

\subsection{Adicionando viés}\label{adicionando-vies}

É comum adicionar a funcão do SVM um termo de viés b, e considerar a
nova função \(f(x_i,w) = w^{T}x_i + b\). Na prática termo de viés pode
ser crucial para ajustar os dados de treino de forma ótima, já que não
há razão que o produto interno \(w^{T}x_i\) devesse ser naturalmente
centrado em zero. Alguns algoritmos de aprendizado do SVM podem estimar
ambos \(w\) e \(b\) diretamente. Porém, outros algoritmos como
\textbf{Stochastic Gradient Descente(SGC)} e \textbf{Stochastic Dual
Coordinate Ascent (SDCA)} não podem (ambos usados pelo LIBLINEAR). Nesse
caso, uma simples solução é adcionar um termo constante \(B > 0\) ao
dado, por exemplo, considere os vetores estendidos:

\(\bar{x_i}=\begin{bmatrix} x_i \\ B \end{bmatrix}\),
\(\bar{w}=\begin{bmatrix} w \\ w_b\end{bmatrix}\).

De modo que função incorpore implicitamente o termo de viés
\(b = Bw_b\):

\begin{equation}
  \bar{w}^T\bar{x_i} = w^Tx_i + Bw_b
\end{equation}

A desvantagem dessa redução é que o termo \(w^2_b\) torna parte do
regularizador do SVM, que encole o viés \(b\) em direção a zero. Esse
efeito pode ser aliviado tomando valor de \(B\) suficientemente grandes,
por causa disso \(||w||^2 >> w^2_b\) e o efeito de encolimento pode ser
ignorado. Infelizmente, fazer \(B\) muito grande faz o problema
numericamente desbalanciado, assim uma troca justa entre encolhimento e
estabilidade é buscada. Tipicamente, isso é obtido normalizando o dado,
para que tenha uma norma Euclidiana unitária e, então, escolhendo
\(B \in [1,10]\).

Referências:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  \url{http://www.vlfeat.org/api/svm-fundamentals.html}
\item
  \url{https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf}
\end{itemize}

\section{Esquema de ponderação
TF-IDF}\label{esquema-de-ponderacao-tf-idf}

Um dos mais populares esquemas de ponderação de termos em recuperação de
informação é baseado na combinação da frequência do termo (TF) e o fator
IDF.

\begin{equation}
    w_{i,j}=\left\{
                \begin{array}{ll}
                  tf \times idf &, f_{i,j} > 0\\
                  0 &, f_{i,j} = 0
                \end{array}
              \right.
\end{equation}

Há várias variações dos fatores TF e IDF, tais variações são mais
adequadas que outras para certos algoritmo de classificação.

\subsection{Variações de TF-IDF}\label{variacoes-de-tf-idf}

A table \ref{tab:tfs} mostra cinco variações do fator TF. O esquema
binário atribui 1 ao TF se o termo ocorre no documento, e 0 caso
contrátio. A frequência pura é o uso da contagem do número de vezes que
o termo ocorre no documento. A normalização log diminui a impacto do
crescimento do frequência \(f_{i,j}\). A normalização \(0.5\) introduz
dois efeitos: 1) ele normaliza o peso pela frequência máxima no
documento and 2) normaliza o peso mantendo-o entre 0.5 e 1. A
normalização \(K\) é simplesmente uma generalização da anterior.

\begin{table}[ht]
\centering
\begin{tabular}{|lc|}
\hline
  Esquema & TF\\ 
\hline
binário & {0,1}\\
frequência pura & $f_{i,j}$\\
normalização log & $1 + \log f_{i,j}$\\
normalização $0.5$ & $0.5 + 0.5  \frac{f_{i,j}}{max_if_{i,j}}$\\
normalização $K$ & $k + (1-K) \frac{f_{i,j}}{max_if_{i,j}}$\\
\hline
\end{tabular}
\caption{Variações do TF}
\label{tab:tfs}
\end{table}

A tabela \ref{tab:idfs} mostra três variações do fator TF. O esquema
unário fixa o valor do IDF como 1 (IDF é ignorado). A frequência inversa
é a formulação padrão para IDF. A frequÊncia inversa suave soma 1 ao
denominador e numerador para evitar comportamento inesperado quando
\(n_i\) atingir valores extremos.

\begin{table}[ht]
\centering
\begin{tabular}{|lc|}
\hline
  Esquema & IDF\\ 
\hline
unária & 1\\
frequência inversa & $\log \frac{N}{n_i}$\\
frequência inversa suave & $\log \frac{N + 1}{n_i + 1}$\\
\hline
\end{tabular}
\caption{Variações do IDF}
\label{tab:idfs}
\end{table}

As combinações das variações de TF e IDF produzem vários esquemas
TF-IDF. Nesse trabalho focaremos apenas no esquema
\((1 + \log f_{i,j}) \times \log \frac{N + 1}{n_i + 1}\).

\iffalse

\begin{table}[ht]
\centering
\begin{tabular}{|cc|}
\hline
  Esquema de ponderação & peso do termo do documento\\ 
\hline
1 & $(1 +  \log f_{i,j}) \times \log \frac{N + 1}{n_i + 1}$\\
2 & $(0.5 + 0.5  \frac{f_{i,j}}{max_if_{i,j}}) \times \log \frac{N + 1}{n_i + 1}$\\
3 & $f_{i,j}$\\
4 & $0.5 + 0.5  \frac{f_{i,j}}{max_if_{i,j}}$\\
\hline
\end{tabular}
\label{tab:tfidfs}
\caption{Variações do TF-IDF}
\end{table}

\fi

\section{Normalização dos documentos}\label{normalizacao-dos-documentos}

Há várias formas de normalizar documentos representados no espaço
vetorial como \textbf{bag-of-words}. Seja \(w_j\) um vetor de pesos,
criado a partir de algum dos esquemas de ponderação mencionado acima,
que representa um documento \(d_j\) no espaço vetorial. Temos as
seguintes formas normalizações utilizadas nesse trabalho:

\begin{table}[ht]
\centering
\begin{tabular}{|cc|}
\hline
  Normalização & fórmula\\ 
\hline
None & $w_j$\\
Max & $\frac{w_j}{\max_i w_{i,j}}$\\
L1 & $\frac{w_j}{\|w_{j}\|_1}$\\
L2 & $\frac{w_j}{\|w_{j}\|_2}$\\
\hline
\end{tabular}
\caption{Normalizações}
\label{tab:norms}
\end{table}

\section{Efeitos dos esquemas de ponderação e normalizacao em
classificação de
texto}\label{efeitos-dos-esquemas-de-ponderacao-e-normalizacao-em-classificacao-de-texto}

Nos experimentos subsequentes, é feito um estudo sobre os efeitos dessas
normalizações e dos esquemas de ponderação quando aplicados a
classificadores vetorias tais como Support Vector Machine (SVM) e K
Nearest Neighbors (KNN).

\subsection{SVM}\label{svm-1}

\% latex table generated in R 3.2.4 by xtable 1.8-0 package \% Thu Apr
28 10:22:27 2016

\begin{table}[ht]
\centering
\begin{tabular}{llllll}
  \hline
V1 & V2 & 20NG & 4UNI & ACM & REUTERS90 \\ 
  \cline{3-6} \hline
\multirow{2}{*}{L2} & microF1 & \bf{90.06 $\pm$  0.43} & \bf{83.48 $\pm$  1.08} & \bf{75.4 $\pm$  0.66} & \bf{68.19 $\pm$  1.15} \\ 
   & macroF1 & \bf{89.93 $\pm$  0.43} & \bf{73.39 $\pm$  2.17} & \bf{63.84 $\pm$  0.55} & \bf{31.95 $\pm$  2.59} \\ 
   \cline{3-6}\multirow{2}{*}{L1} & microF1 & \bf{89.8 $\pm$  0.4} & 78.23 $\pm$  1.49 & \bf{75.31 $\pm$  0.74} & \bf{68.25 $\pm$  1.2} \\ 
   & macroF1 & \bf{89.59 $\pm$  0.43} & 67.47 $\pm$  3.01 & \bf{62.33 $\pm$  1.76} & \bf{31.37 $\pm$  2.22} \\ 
   \cline{3-6}\multirow{2}{*}{MAX} & microF1 & 88.35 $\pm$  0.37 & 81.36 $\pm$  1.01 & 73.82 $\pm$  0.78 & \bf{67.6 $\pm$  1.1} \\ 
   & macroF1 & 88.3 $\pm$  0.38 & 68.01 $\pm$  2.39 & \bf{62.55 $\pm$  1.53} & \bf{31.73 $\pm$  3.13} \\ 
   \cline{3-6}\multirow{2}{*}{NONE} & microF1 & 83.47 $\pm$  0.46 & 80.55 $\pm$  0.72 & 71.34 $\pm$  1.01 & 66.6 $\pm$  1.06 \\ 
   & macroF1 & 83.37 $\pm$  0.42 & \bf{71.04 $\pm$  2.06} & 61.08 $\pm$  0.67 & \bf{31.68 $\pm$  3.32} \\ 
   \cline{3-6}\end{tabular}
\caption{Comparação entre as normalizações aplicada ao calssificador SVM} 
\end{table}

Como pode-se observar, a normalização tem um papel fundamental no
desempenho do SVM. A normalização \(L2\) teve o melhor desempenho em
todos os conjuntos de dados testados.

\subsection{KNN}\label{knn}

\% latex table generated in R 3.2.4 by xtable 1.8-0 package \% Thu Apr
28 10:22:36 2016

\begin{table}[ht]
\centering
\begin{tabular}{llllll}
  \hline
V1 & V2 & 20NG & 4UNI & ACM & REUTERS90 \\ 
  \cline{3-6} \hline
\multirow{2}{*}{KNN-L2} & microF1 & \bf{87.39 $\pm$  0.68} & \bf{75.51 $\pm$  1.25} & \bf{70.67 $\pm$  1.1} & \bf{69.39 $\pm$  1.46} \\ 
   & macroF1 & \bf{87.1 $\pm$  0.66} & \bf{60.15 $\pm$  1.26} & \bf{55.39 $\pm$  0.96} & \bf{34.23 $\pm$  2.75} \\ 
   \cline{3-6}\multirow{2}{*}{KNN-NONE} & microF1 & \bf{87.45 $\pm$  0.67} & \bf{75.73 $\pm$  1.2} & \bf{71.06 $\pm$  1.03} & \bf{68.13 $\pm$  1.01} \\ 
   & macroF1 & \bf{87.15 $\pm$  0.65} & \bf{60.02 $\pm$  0.8} & \bf{55.82 $\pm$  0.92} & \bf{31.53 $\pm$  3.42} \\ 
   \cline{3-6}\multirow{2}{*}{KNN-L1} & microF1 & \bf{87.46 $\pm$  0.69} & \bf{75.74 $\pm$  0.86} & \bf{71.02 $\pm$  1.08} & 67.8 $\pm$  1.02 \\ 
   & macroF1 & \bf{87.16 $\pm$  0.66} & \bf{60.29 $\pm$  0.55} & \bf{55.83 $\pm$  1.15} & \bf{30.91 $\pm$  2.7} \\ 
   \cline{3-6}\multirow{2}{*}{KNN-MAX} & microF1 & \bf{87.53 $\pm$  0.69} & \bf{75.63 $\pm$  0.94} & \bf{70.99 $\pm$  0.96} & \bf{68.07 $\pm$  1.07} \\ 
   & macroF1 & \bf{87.22 $\pm$  0.66} & \bf{60.34 $\pm$  1.36} & \bf{55.85 $\pm$  0.97} & 29.93 $\pm$  2.48 \\ 
   \cline{3-6}\end{tabular}
\caption{Comparação entre as normalizações aplicada ao calssificador KNN} 
\end{table}

Como pode-se observar, para o KNN a normalização teve pouco efeito sobre
a efetividade do classificador. A normalização \(L2\) empata
estatisticamente com o esquema de ponderação sem normalização(None), que
praticamente empatou com as outras normalizações.

\chapter{Avaliação experimental}

\section{Caso de Estudo: Classificação Automática de
Texto}\label{caso-de-estudo-classificacao-automatica-de-texto}

\subsection{Resultados e discussões}\label{resultados-e-discussoes}

A Tabela \ref{tab:base} sumariza os resultados da avaliação empirica de
algums algoritmos estado-da-arte para classificação de texto e os
algoritmos propostos baseado na \textbf{Extremelly Randomized
Tree}(doravante Extra-trees).

Primeiro aspecto que pode ser observado é que as \emph{Extra-Trees} por
si só melhoram a eficácia da \emph{Random Forest} em dois conjuntos de
dados(empatando nos outros dois). Esse resultado comprova que
\emph{Extra-Trees} também são mais robustas a ruídos e atributos
irrelevante como mostrado em \citep{GEURTS2006}, além disso, mostra que
isso se mantém quando aplicadas a tarefas de classificação de texto.
Todavia, está longe de figurar o cojunto dos melhores classificadores
dentre os algoritmos analizados. O SVM continua sendo o melhor
classificador, sendo o melhor classificador em todos os 4 conjuntos de
dados. Isso não é supresa, já que o SVM é bom para aprender quando
aplicado a dados de alta dimensionalidade e com natural robustês para
atributos ruidosos e irrelevantes.

Logo em seguida vem \emph{BERT (Boosted Extremelly Randomized Trees)}, o
algoritmo é uma extensão do \emph{BROOF} proposto em
\citep{Salles:2015:BEO:2766462.2767747}. Os resultados obtidos com a
abordagem comprova nossa intuição de que a simples substituição da RF
pela Extra-Trees traria ganhos expressivos no poder de generalização do
\emph{BROOF}, tornando a técnica ainda mais competitiva se comparada a
outros classificadores.

Outra proposta foi a substituíção da RF pela \emph{Extra-Trees} no
algorimo \emph{LazyNN\_RF}, na esperação de ganho no poder de
generalização do mesmo. É nítido que o uso das Extra-Trees no algoritmo
LXT proporcionou um ganho sobretudo nos conjuntos de dados 20NG e
REUTERS90, quando comparado ao algoritmo Lazy.

Esses resultados reforçam que as Extra-Trees são mais robustas a ruídos
que as RF, todavia estão aquém se comparadas a capacidade do SVM de
lidar com dados de alta dimensionalidade e ruidosos. Porém, com uso das
técnicas apresentadas em \citep{Salles:Proposal:2015} pode-se aumentar a
capacidade desses algoritmos de lidar com dados ruidosos tarnando-os
assim mais competitivos ou muitas vezes melhores que a os algoritmos a
presentados na Tabela \ref{tab:base}.

\% latex table generated in R 3.2.4 by xtable 1.8-0 package \% Thu Apr
28 17:53:01 2016

\begin{table}[ht]
\centering
\begin{tabular}{llllll}
  \hline
V1 & V2 & 20NG & 4UNI & ACM & REUTERS90 \\ 
  \cline{3-6} \hline
\multirow{2}{*}{SVM} & microF1 & \bf{90.06 $\pm$  0.43} & \bf{83.48 $\pm$  1.08} & \bf{75.4 $\pm$  0.66} & \bf{68.19 $\pm$  1.15} \\ 
   & macroF1 & \bf{89.93 $\pm$  0.43} & \bf{73.39 $\pm$  2.17} & \bf{63.84 $\pm$  0.55} & \bf{31.95 $\pm$  2.59} \\ 
   \cline{3-6}\multirow{2}{*}{BERT} & microF1 & 88.93 $\pm$  0.39 & \bf{84.61 $\pm$  0.98} & \bf{74.8 $\pm$  0.59} & \bf{67.33 $\pm$  0.72} \\ 
   & macroF1 & 88.59 $\pm$  0.5 & \bf{73.61 $\pm$  1.85} & \bf{62.1 $\pm$  0.99} & \bf{29.24 $\pm$  1.4} \\ 
   \cline{3-6}\multirow{2}{*}{BROOF} & microF1 & 87.96 $\pm$  0.24 & \bf{84.41 $\pm$  1.07} & 73.35 $\pm$  0.79 & \bf{66.79 $\pm$  0.97} \\ 
   & macroF1 & 87.44 $\pm$  0.28 & \bf{73.23 $\pm$  1.1} & 60.76 $\pm$  0.8 & \bf{28.48 $\pm$  2.17} \\ 
   \cline{3-6}\multirow{2}{*}{LAZY} & microF1 & 87.96 $\pm$  0.37 & \bf{82.34 $\pm$  0.61} & \bf{74.02 $\pm$  0.79} & \bf{66.3 $\pm$  1.07} \\ 
   & macroF1 & 87.39 $\pm$  0.37 & 68.33 $\pm$  1.6 & 59.46 $\pm$  1.35 & 26.61 $\pm$  2.12 \\ 
   \cline{3-6}\multirow{2}{*}{KNN} & microF1 & 87.53 $\pm$  0.69 & 75.63 $\pm$  0.94 & 70.99 $\pm$  0.96 & \bf{68.07 $\pm$  1.07} \\ 
   & macroF1 & 87.22 $\pm$  0.66 & 60.34 $\pm$  1.36 & 55.85 $\pm$  0.97 & \bf{29.93 $\pm$  2.48} \\ 
   \cline{3-6}\multirow{2}{*}{NB} & microF1 & 88.99 $\pm$  0.54 & 62.63 $\pm$  1.7 & 73.54 $\pm$  0.71 & 65.32 $\pm$  1.13 \\ 
   & macroF1 & 88.68 $\pm$  0.55 & 51.38 $\pm$  3.19 & 58.03 $\pm$  0.85 & \bf{27.86 $\pm$  0.79} \\ 
   \cline{3-6}\multirow{2}{*}{XT} & microF1 & 85.94 $\pm$  0.23 & 81.66 $\pm$  1.03 & 71.94 $\pm$  0.66 & 64.33 $\pm$  0.86 \\ 
   & macroF1 & 85.57 $\pm$  0.22 & 65.44 $\pm$  2.41 & 57.4 $\pm$  1.13 & 24.47 $\pm$  2.22 \\ 
   \cline{3-6}\multirow{2}{*}{LXT} & microF1 & 88.39 $\pm$  0.51 & 81.24 $\pm$  0.71 & 69.63 $\pm$  0.91 & 65.92 $\pm$  0.82 \\ 
   & macroF1 & 88.05 $\pm$  0.44 & 66.89 $\pm$  1.23 & 57.33 $\pm$  1.48 & 26.71 $\pm$  2.53 \\ 
   \cline{3-6}\multirow{2}{*}{RF} & microF1 & 83.64 $\pm$  0.29 & 81.52 $\pm$  1 & 71.05 $\pm$  0.31 & 63.92 $\pm$  0.81 \\ 
   & macroF1 & 83.08 $\pm$  0.35 & 65.44 $\pm$  1.91 & 56.56 $\pm$  0.45 & 24.36 $\pm$  1.98 \\ 
   \cline{3-6}\end{tabular}
\caption{Comparação entres métodos de base} 
\label{tab:base}
\end{table}

\subsubsection{Stacking}\label{stacking-1}

Nessa seção contrastamos o aumento do poder de generalização ao
combinarmos técnicas altamente eficázes, tais como LazyNN\_RF, BROOF,
BERT e LazyExtraTrees(LXT), com o ganho proporcionado pela combinação de
alguns classificadores estado-da-arte para classificação automática de
texto.

\begin{longtable}[c]{@{}ll@{}}
\caption{Legenda para os stacking. Os classificadores reportados na
descrição são todos do nível base, para todos os stackings foi utilizado
RF como classificador do meta-nível.}\tabularnewline
\toprule
Stacking & Descrição\tabularnewline
\midrule
\endfirsthead
\toprule
Stacking & Descrição\tabularnewline
\midrule
\endhead
COMB1 & BROOF + Lazy\tabularnewline
COMB2 & BERT + LXT\tabularnewline
COMB3 & BROOF + Lazy + BERT + LXT\tabularnewline
COMBSOTA & SVM + KNN + XT + RF + NB\tabularnewline
COMBALL & Stacking de todos\tabularnewline
\bottomrule
\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\itemsep1pt\parskip0pt\parsep0pt
\item
  temos um a abordagem combinando stacking, bagging (arvores
  tradicionais e LAZY) e boosting (BROOF e BERT)
\item
  os resulatdos de stacking so são bons qdo os novos métodos de
  florestas entram (COMBSOTA não eh tao bom)
\item
  ha' um ganho em combinar tudo mas 5) nao precisa combinar tudo
  praobter bons resuatdos, o stacking de florestas jah eh competitivo
\end{enumerate}

\% latex table generated in R 3.2.4 by xtable 1.8-0 package \% Thu Apr
28 18:21:38 2016

\begin{table}[ht]
\centering
\begin{tabular}{llllll}
  \hline
V1 & V2 & 20NG & 4UNI & ACM & REUTERS90 \\ 
  \cline{3-6} \hline
\multirow{2}{*}{COMBALL} & microF1 & \bf{91.67 $\pm$  0.44} & \bf{86.74 $\pm$  1.17} & \bf{78.46 $\pm$  0.72} & \bf{80.02 $\pm$  1.24} \\ 
   & macroF1 & \bf{91.43 $\pm$  0.42} & \bf{79.45 $\pm$  2.23} & \bf{63.72 $\pm$  1.01} & \bf{37.84 $\pm$  3.14} \\ 
   \cline{3-6}\multirow{2}{*}{COMB3} & microF1 & 90.63 $\pm$  0.57 & \bf{86.79 $\pm$  0.86} & \bf{77.34 $\pm$  0.6} & \bf{79 $\pm$  1.14} \\ 
   & macroF1 & 90.4 $\pm$  0.57 & \bf{79.63 $\pm$  1.91} & \bf{62.91 $\pm$  0.92} & \bf{33.93 $\pm$  2.97} \\ 
   \cline{3-6}\multirow{2}{*}{COMB2} & microF1 & 90.2 $\pm$  0.51 & \bf{86.54 $\pm$  1.06} & 76.88 $\pm$  0.55 & \bf{78.25 $\pm$  1.17} \\ 
   & macroF1 & 89.95 $\pm$  0.52 & \bf{79.41 $\pm$  1.63} & \bf{62.66 $\pm$  0.81} & \bf{32.86 $\pm$  2.23} \\ 
   \cline{3-6}\multirow{2}{*}{COMBSOTA} & microF1 & 90.65 $\pm$  0.45 & \bf{84.95 $\pm$  1.15} & \bf{77.78 $\pm$  0.73} & 74.63 $\pm$  1 \\ 
   & macroF1 & 90.42 $\pm$  0.44 & \bf{75.96 $\pm$  1.78} & \bf{63.04 $\pm$  0.85} & 27.66 $\pm$  0.88 \\ 
   \cline{3-6}\multirow{2}{*}{COMB1} & microF1 & 89.32 $\pm$  0.42 & \bf{86.52 $\pm$  1.18} & 76.74 $\pm$  0.73 & 77.22 $\pm$  1.14 \\ 
   & macroF1 & 89.01 $\pm$  0.44 & \bf{78.66 $\pm$  1.9} & \bf{62.2 $\pm$  1.01} & 31.71 $\pm$  2.7 \\ 
   \cline{3-6}\multirow{2}{*}{BERT} & microF1 & 88.93 $\pm$  0.39 & \bf{84.61 $\pm$  0.98} & 74.8 $\pm$  0.59 & 67.33 $\pm$  0.72 \\ 
   & macroF1 & 88.59 $\pm$  0.5 & 73.61 $\pm$  1.85 & \bf{62.1 $\pm$  0.99} & 29.24 $\pm$  1.4 \\ 
   \cline{3-6}\multirow{2}{*}{SVM-L2} & microF1 & 90.06 $\pm$  0.43 & 83.48 $\pm$  1.08 & 75.4 $\pm$  0.66 & 68.19 $\pm$  1.15 \\ 
   & macroF1 & 89.93 $\pm$  0.43 & 73.39 $\pm$  2.17 & \bf{63.84 $\pm$  0.55} & 31.95 $\pm$  2.59 \\ 
   \cline{3-6}\end{tabular}
\caption{Comparação entre os métodos de stackings} 
\label{tab:stacking}
\end{table}

\renewcommand\refname{References}
\bibliography{references}

\end{document}
