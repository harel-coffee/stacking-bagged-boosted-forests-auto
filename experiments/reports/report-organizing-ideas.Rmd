---
title: "Stacking utilizado nos experimentos"
author: "Raphael Rodrigues Campos"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    keep_tex: yes
    citation_package: natbib
    number_sections: true
documentclass: article
bibliography: references.bib
header-includes: \usepackage{multirow} \usepackage[utf8]{inputenc}  \usepackage{mathtools} \usepackage[portuguese]{babel}
---

\chapter{Overview}
 

```{r child='concepts/stacking.Rmd'}
```

```{r child='concepts/svm.Rmd'}
```
# Esquema de ponderação TF-IDF 

Um dos mais populares esquemas de ponderação de termos em recuperação de informação é baseado na combinação da frequência do termo (TF) e o fator IDF.

\begin{equation}
    w_{i,j}=\left\{
                \begin{array}{ll}
                  tf \times idf &, f_{i,j} > 0\\
                  0 &, f_{i,j} = 0
                \end{array}
              \right.
\end{equation}

Há várias variações dos fatores TF e IDF, tais variações são mais adequadas que outras para certos algoritmo de classificação. 

## Variações de TF-IDF

A table \ref{tab:tfs} mostra cinco variações do fator TF. O esquema binário atribui 1 ao TF se o termo ocorre no documento, e 0 caso contrátio. A frequência pura é o uso da contagem do número de vezes que o termo ocorre no documento. A normalização log diminui a impacto do crescimento do frequência $f_{i,j}$. A normalização $0.5$ introduz dois efeitos: 1) ele normaliza o peso pela frequência máxima no documento and 2) normaliza o peso mantendo-o entre 0.5 e 1. A normalização $K$ é simplesmente uma generalização da anterior.

\begin{table}[ht]
\centering
\begin{tabular}{|lc|}
\hline
  Esquema & TF\\ 
\hline
binário & {0,1}\\
frequência pura & $f_{i,j}$\\
normalização log & $1 + \log f_{i,j}$\\
normalização $0.5$ & $0.5 + 0.5  \frac{f_{i,j}}{max_if_{i,j}}$\\
normalização $K$ & $k + (1-K) \frac{f_{i,j}}{max_if_{i,j}}$\\
\hline
\end{tabular}
\caption{Variações do TF}
\label{tab:tfs}
\end{table}

A tabela \ref{tab:idfs} mostra três variações do fator TF. O esquema unário fixa o valor do IDF como 1 (IDF é ignorado). A frequência inversa  é a formulação padrão para IDF. A frequÊncia inversa suave soma 1 ao denominador e numerador para evitar comportamento inesperado quando $n_i$ atingir valores extremos.

\begin{table}[ht]
\centering
\begin{tabular}{|lc|}
\hline
  Esquema & IDF\\ 
\hline
unária & 1\\
frequência inversa & $\log \frac{N}{n_i}$\\
frequência inversa suave & $\log \frac{N + 1}{n_i + 1}$\\
\hline
\end{tabular}
\caption{Variações do IDF}
\label{tab:idfs}
\end{table}

As combinações das variações de TF e IDF produzem vários esquemas TF-IDF. Nesse trabalho focaremos apenas no esquema $(1 +  \log f_{i,j}) \times \log \frac{N + 1}{n_i + 1}$.

\iffalse
\begin{table}[ht]
\centering
\begin{tabular}{|cc|}
\hline
  Esquema de ponderação & peso do termo do documento\\ 
\hline
1 & $(1 +  \log f_{i,j}) \times \log \frac{N + 1}{n_i + 1}$\\
2 & $(0.5 + 0.5  \frac{f_{i,j}}{max_if_{i,j}}) \times \log \frac{N + 1}{n_i + 1}$\\
3 & $f_{i,j}$\\
4 & $0.5 + 0.5  \frac{f_{i,j}}{max_if_{i,j}}$\\
\hline
\end{tabular}
\label{tab:tfidfs}
\caption{Variações do TF-IDF}
\end{table}
\fi

# Normalização dos documentos

Há várias formas de normalizar documentos representados no espaço vetorial como **bag-of-words**. Seja $w_j$ um vetor de pesos, criado a partir de algum dos esquemas de ponderação mencionado acima, que representa um documento $d_j$ no espaço vetorial. Temos as seguintes formas normalizações utilizadas nesse trabalho:  

\begin{table}[ht]
\centering
\begin{tabular}{|cc|}
\hline
  Normalização & fórmula\\ 
\hline
None & $w_j$\\
Max & $\frac{w_j}{\max_i w_{i,j}}$\\
L1 & $\frac{w_j}{\|w_{j}\|_1}$\\
L2 & $\frac{w_j}{\|w_{j}\|_2}$\\
\hline
\end{tabular}
\caption{Normalizações}
\label{tab:norms}
\end{table}


# Efeitos dos esquemas de ponderação e normalizacao em classificação de texto

Nos experimentos subsequentes, é feito um estudo sobre os efeitos dessas normalizações e dos esquemas de ponderação quando aplicados a classificadores vetorias tais como Support Vector Machine (SVM) e K Nearest Neighbors (KNN).

## SVM
```{r, message=FALSE, warning=FALSE, echo=FALSE}

```

```{r, message=FALSE, warning=FALSE, echo=FALSE, cache=T,  results="asis", echo=F}
build_table("~/Documents/Master\ Degree/Master\ Project/Implementation/LazyNN_RF/release/results/results_norm/svm/tf",
            "Comparação entre as normalizações aplicada ao calssificador SVM")

#build_table("~/Documents/Master\ Degree/Master\ Project/Implementation/LazyNN_RF/release/results/results_norm/max_tf",
#            "Comparação entre as normalizações aplicadas ao esquema de ponderação 2")

#build_table("~/Documents/Master\ Degree/Master\ Project/Implementation/LazyNN_RF/release/results/results_norm/best",
#            "Comparação entre todos os métodos")

```

Como pode-se observar, a normalização tem um papel fundamental no desempenho do SVM. A normalização $L2$ teve o melhor desempenho em todos os conjuntos de dados testados.

## KNN

```{r, message=FALSE, warning=FALSE, echo=FALSE, cache=T,  results="asis", echo=F}
build_table("~/Documents/Master\ Degree/Master\ Project/Implementation/LazyNN_RF/release/results/results_norm/knn/tf",
            "Comparação entre as normalizações aplicada ao calssificador KNN")
```

Como pode-se observar, para o KNN a normalização teve pouco efeito sobre a efetividade do classificador. A normalização $L2$ empata estatisticamente com o esquema de ponderação sem normalização(None), que praticamente empatou com as outras normalizações.


\chapter{Avaliação experimental}
```{r child='exp/methods-effectiveness-report.Rmd'}
```


# References