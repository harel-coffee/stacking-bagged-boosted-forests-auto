---
title: "Stacking utilizado nos experimentos"
author: "Raphael Rodrigues Campos"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    keep_tex: yes
    citation_package: natbib
    number_sections: true
documentclass: book
bibliography: references.bib
header-includes: \usepackage{multirow} \usepackage[utf8]{inputenc}  \usepackage{mathtools} \usepackage[portuguese]{babel}
---

\chapter{Overview}
 

```{r child='concepts/stacking.Rmd'}
```

```{r child='concepts/svm.Rmd'}
```

## Adicionando viés

É comum adicionar a funcão do SVM um termo de viés b, e considerar a nova função $f(x_i,w) = w^{T}x_i + b$. Na prática termo de viés pode ser crucial para ajustar os dados de treino de forma ótima, já que não há razão que o produto interno $w^{T}x_i$ devesse ser naturalmente centrado em zero. Alguns algoritmos de aprendizado do SVM podem estimar ambos $w$ e $b$ diretamente. Porém, outros algoritmos como **Stochastic Gradient Descente(SGC)** e **Stochastic Dual Coordinate Ascent (SDCA)** não podem (ambos usados pelo LIBLINEAR). Nesse caso, uma simples solução é adcionar um termo constante $B > 0$ ao dado, por exemplo, considere os vetores estendidos:

$\bar{x_i}=\begin{bmatrix} x_i \\ B \end{bmatrix}$, $\bar{w}=\begin{bmatrix} w \\ w_b\end{bmatrix}$.

De modo que função incorpore implicitamente o termo de viés $b = Bw_b$: 
\begin{equation}
  \bar{w}^T\bar{x_i} = w^Tx_i + Bw_b
\end{equation}

A desvantagem dessa redução é que o termo $w^2_b$ torna parte do regularizador do SVM, que encole o viés $b$ em direção a zero. Esse efeito pode ser aliviado tomando valor de $B$ suficientemente grandes, por causa disso $||w||^2 >> w^2_b$ e o efeito de encolimento pode ser ignorado.
Infelizmente, fazer $B$ muito grande faz o problema numericamente desbalanciado, assim uma troca justa entre encolhimento e estabilidade é buscada. Tipicamente, isso é obtido normalizando o dado, para que tenha uma norma Euclidiana unitária e, então, escolhendo $B \in [1,10]$.

Referências:

  - http://www.vlfeat.org/api/svm-fundamentals.html
  - https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf 


# Esquema de ponderação TF-IDF 

Um dos mais populares esquemas de ponderação de termos em recuperação de informação é baseado na combinação da frequência do termo (TF) e o fator IDF.

\begin{equation}
    w_{i,j}=\left\{
                \begin{array}{ll}
                  tf \times idf &, f_{i,j} > 0\\
                  0 &, f_{i,j} = 0
                \end{array}
              \right.
\end{equation}

Há várias variações dos fatores TF e IDF, tais variações são mais adequadas que outras para certos algoritmo de classificação. 

## Variações de TF-IDF

A table \ref{tab:tfs} mostra cinco variações do fator TF. O esquema binário atribui 1 ao TF se o termo ocorre no documento, e 0 caso contrátio. A frequência pura é o uso da contagem do número de vezes que o termo ocorre no documento. A normalização log diminui a impacto do crescimento do frequência $f_{i,j}$. A normalização $0.5$ introduz dois efeitos: 1) ele normaliza o peso pela frequência máxima no documento and 2) normaliza o peso mantendo-o entre 0.5 e 1. A normalização $K$ é simplesmente uma generalização da anterior.

\begin{table}[ht]
\centering
\begin{tabular}{|lc|}
\hline
  Esquema & TF\\ 
\hline
binário & {0,1}\\
frequência pura & $f_{i,j}$\\
normalização log & $1 + \log f_{i,j}$\\
normalização $0.5$ & $0.5 + 0.5  \frac{f_{i,j}}{max_if_{i,j}}$\\
normalização $K$ & $k + (1-K) \frac{f_{i,j}}{max_if_{i,j}}$\\
\hline
\end{tabular}
\caption{Variações do TF}
\label{tab:tfs}
\end{table}

A tabela \ref{tab:idfs} mostra três variações do fator TF. O esquema unário fixa o valor do IDF como 1 (IDF é ignorado). A frequência inversa  é a formulação padrão para IDF. A frequÊncia inversa suave soma 1 ao denominador e numerador para evitar comportamento inesperado quando $n_i$ atingir valores extremos.

\begin{table}[ht]
\centering
\begin{tabular}{|lc|}
\hline
  Esquema & IDF\\ 
\hline
unária & 1\\
frequência inversa & $\log \frac{N}{n_i}$\\
frequência inversa suave & $\log \frac{N + 1}{n_i + 1}$\\
\hline
\end{tabular}
\caption{Variações do IDF}
\label{tab:idfs}
\end{table}

As combinações das variações de TF e IDF produzem vários esquemas TF-IDF. Nesse trabalho focaremos apenas no esquema $(1 +  \log f_{i,j}) \times \log \frac{N + 1}{n_i + 1}$.

\iffalse
\begin{table}[ht]
\centering
\begin{tabular}{|cc|}
\hline
  Esquema de ponderação & peso do termo do documento\\ 
\hline
1 & $(1 +  \log f_{i,j}) \times \log \frac{N + 1}{n_i + 1}$\\
2 & $(0.5 + 0.5  \frac{f_{i,j}}{max_if_{i,j}}) \times \log \frac{N + 1}{n_i + 1}$\\
3 & $f_{i,j}$\\
4 & $0.5 + 0.5  \frac{f_{i,j}}{max_if_{i,j}}$\\
\hline
\end{tabular}
\label{tab:tfidfs}
\caption{Variações do TF-IDF}
\end{table}
\fi

# Normalização dos documentos

Há várias formas de normalizar documentos representados no espaço vetorial como **bag-of-words**. Seja $w_j$ um vetor de pesos, criado a partir de algum dos esquemas de ponderação mencionado acima, que representa um documento $d_j$ no espaço vetorial. Temos as seguintes formas normalizações utilizadas nesse trabalho:  

\begin{table}[ht]
\centering
\begin{tabular}{|cc|}
\hline
  Normalização & fórmula\\ 
\hline
None & $w_j$\\
Max & $\frac{w_j}{\max_i w_{i,j}}$\\
L1 & $\frac{w_j}{\|w_{j}\|_1}$\\
L2 & $\frac{w_j}{\|w_{j}\|_2}$\\
\hline
\end{tabular}
\caption{Normalizações}
\label{tab:norms}
\end{table}


# Efeitos dos esquemas de ponderação e normalizacao em classificação de texto

Nos experimentos subsequentes, é feito um estudo sobre os efeitos dessas normalizações e dos esquemas de ponderação quando aplicados a classificadores vetorias tais como Support Vector Machine (SVM) e K Nearest Neighbors (KNN).

## SVM
```{r, message=FALSE, warning=FALSE, echo=FALSE}

```

```{r, message=FALSE, warning=FALSE, echo=FALSE, cache=T,  results="asis", echo=F}
build_table("~/Documents/Master\ Degree/Master\ Project/Implementation/LazyNN_RF/release/results/results_norm/svm/tf",
            "Comparação entre as normalizações aplicada ao calssificador SVM")

#build_table("~/Documents/Master\ Degree/Master\ Project/Implementation/LazyNN_RF/release/results/results_norm/max_tf",
#            "Comparação entre as normalizações aplicadas ao esquema de ponderação 2")

#build_table("~/Documents/Master\ Degree/Master\ Project/Implementation/LazyNN_RF/release/results/results_norm/best",
#            "Comparação entre todos os métodos")

```

Como pode-se observar, a normalização tem um papel fundamental no desempenho do SVM. A normalização $L2$ teve o melhor desempenho em todos os conjuntos de dados testados.

## KNN

```{r, message=FALSE, warning=FALSE, echo=FALSE, cache=T,  results="asis", echo=F}
build_table("~/Documents/Master\ Degree/Master\ Project/Implementation/LazyNN_RF/release/results/results_norm/knn/tf",
            "Comparação entre as normalizações aplicada ao calssificador KNN")
```

Como pode-se observar, para o KNN a normalização teve pouco efeito sobre a efetividade do classificador. A normalização $L2$ empata estatisticamente com o esquema de ponderação sem normalização(None), que praticamente empatou com as outras normalizações.


\chapter{Avaliação experimental}

```{r child='exp/methods-effectiveness-report.Rmd'}
```

# References